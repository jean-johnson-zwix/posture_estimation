# -*- coding: utf-8 -*-
"""posture_estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fAB9o9tAhpJ1VCO7Wczv6yupDIEFAH8g
"""

!pip -q install mediapipe opencv-python gradio fastrtc

!pip install --upgrade mediapipe==0.10.21

"""# MediaPipe Landmark Detection

## Download the model
"""

!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_lite.task

from google.colab import drive
drive.mount('/content/drive')

"""## Phase 1: Posture Detection

### Create Landmarker for video
"""

import mediapipe as mp

# VIDEO Model Landmarker
BaseOptions = mp.tasks.BaseOptions
PoseLandmarker = mp.tasks.vision.PoseLandmarker
PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions
VisionRunningMode = mp.tasks.vision.RunningMode

# Create a pose landmarker instance with the video mode:
options = PoseLandmarkerOptions(
    base_options=BaseOptions(model_asset_path='/content/pose_landmarker.task'),
    running_mode=VisionRunningMode.VIDEO)

landmarker = PoseLandmarker.create_from_options(options)

"""### Draw Landmark"""

from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
import numpy as np


def draw_landmarks_on_image(rgb_image, detection_result, thickness=4, circle_radius=6):
  pose_landmarks_list = detection_result.pose_landmarks
  annotated_image = np.copy(rgb_image)

  for pose_landmarks in pose_landmarks_list:
      pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
      pose_landmarks_proto.landmark.extend([
          landmark_pb2.NormalizedLandmark(x=l.x, y=l.y, z=l.z) for l in pose_landmarks
      ])

      landmark_spec = solutions.drawing_utils.DrawingSpec(thickness=thickness, circle_radius=circle_radius)
      connection_spec = solutions.drawing_utils.DrawingSpec(thickness=thickness)

      solutions.drawing_utils.draw_landmarks(
          image=annotated_image,
          landmark_list=pose_landmarks_proto,
          connections=solutions.pose.POSE_CONNECTIONS,
          landmark_drawing_spec=landmark_spec,
          connection_drawing_spec=connection_spec,
      )

  return annotated_image

"""### Compute Posture Angles"""

import cv2
import numpy as np
import math

def calculate_angle(point1, point2, vertex):
    try:
        a = np.array(point1)
        b = np.array(vertex)
        c = np.array(point2)

        ba = a - b
        bc = c - b

        # Check for zero vectors
        if np.all(ba == 0) or np.all(bc == 0):
            return 0

        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))
        angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))
        return np.degrees(angle)
    except Exception as e:
        print(f"Error calculating angle: {e}")
        return 0

def get_perpendicular_line(point1, point2, length=200):
    try:
        # Get midpoint
        mid_x = (point1[0] + point2[0]) // 2
        mid_y = (point1[1] + point2[1]) // 2

        # Calculate direction vector
        dx = point2[0] - point1[0]
        dy = point2[1] - point1[1]

        # Check for zero magnitude
        magnitude = math.sqrt(dx*dx + dy*dy)
        if magnitude < 1e-6:
            return (mid_x, mid_y), (mid_x, mid_y - length), (mid_x, mid_y + length)

        # Get perpendicular vector (-dy, dx)
        perp_dx = -dy
        perp_dy = dx

        # Normalize and scale
        perp_dx = int((perp_dx / magnitude) * length)
        perp_dy = int((perp_dy / magnitude) * length)

        perp_point1 = (mid_x + perp_dx, mid_y + perp_dy)
        perp_point2 = (mid_x - perp_dx, mid_y - perp_dy)

        return (mid_x, mid_y), perp_point1, perp_point2
    except Exception as e:
        print(f"Error calculating perpendicular line: {e}")
        return (point1[0], point1[1]), point1, point2

def add_legend(
    image_bgr,
    items,
    margin=20,
    padding=14,
    line_gap=10,
    font=cv2.FONT_HERSHEY_SIMPLEX,
    font_scale=0.9,          # bigger text
    text_thickness=2,
    swatch_len=36,
    swatch_thickness=6,
    bg_color=(25, 25, 25),   # dark bg
    bg_alpha=0.65,           # transparency
    shadow_offset=6,
    shadow_alpha=0.35
):
    h, w = image_bgr.shape[:2]
    if not items:
        return

    text_sizes = [cv2.getTextSize(t, font, font_scale, text_thickness) for t, _ in items]
    text_ws = [ts[0][0] for ts in text_sizes]
    text_hs = [ts[0][1] for ts in text_sizes]
    baselines = [ts[1] for ts in text_sizes]

    line_h = max(text_hs) + line_gap
    box_w = padding * 3 + max(text_ws) + swatch_len
    box_h = padding * 2 + line_h * len(items)

    x0 = w - margin - box_w
    y0 = margin
    x1 = x0 + box_w
    y1 = y0 + box_h

    overlay = image_bgr.copy()
    cv2.rectangle(
        overlay,
        (x0 + shadow_offset, y0 + shadow_offset),
        (x1 + shadow_offset, y1 + shadow_offset),
        (0, 0, 0),
        -1
    )
    image_bgr[:] = cv2.addWeighted(overlay, shadow_alpha, image_bgr, 1 - shadow_alpha, 0)

    overlay = image_bgr.copy()
    cv2.rectangle(overlay, (x0, y0), (x1, y1), bg_color, -1)
    image_bgr[:] = cv2.addWeighted(overlay, bg_alpha, image_bgr, 1 - bg_alpha, 0)

    y = y0 + padding + max(text_hs)
    for (text, color), th, base in zip(items, text_hs, baselines):
        cv2.putText(
            image_bgr,
            text,
            (x0 + padding, y),
            font,
            font_scale,
            (255, 255, 255),
            text_thickness,
            cv2.LINE_AA
        )

        y_mid = y - th // 2
        sx1 = x1 - padding
        sx0 = sx1 - swatch_len
        cv2.line(image_bgr, (sx0, y_mid), (sx1, y_mid), color, swatch_thickness)

        y += line_h

def calculate_midpoint(point1, point2):
    return ((point1[0] + point2[0])//2, (point1[1] + point2[1])//2)

def overlay_posture_angles(image_rgb, pose_landmarks):

    h, w = image_rgb.shape[:2]

    def to_px(i):
        lm = pose_landmarks[i]
        return (int(lm.x * w), int(lm.y * h))

    try:
		    # Extract key landmarks using the provided indices
        nose = to_px(0)
        left_shoulder  = to_px(11)
        right_shoulder = to_px(12)
        left_hip  = to_px(23)
        right_hip = to_px(24)
        left_knee  = to_px(25)
        right_knee = to_px(26)

		    # Calculate additional points (33: hip_mid, 34: neck_mid)
        hip_mid  = calculate_midpoint(left_hip, right_hip)
        neck_mid = calculate_midpoint(left_shoulder, right_shoulder)

		    # Calculate body angle for hips, neck and knees
        knee_mid = calculate_midpoint(left_knee, right_knee)
        hip_up   = (hip_mid[0],  hip_mid[1] - 100)
        hip_down = (hip_mid[0],  hip_mid[1] + 100)
        neck_up  = (neck_mid[0], neck_mid[1] - 100)

        # Determine if sitting/standing based on leg angle
        leg_angle = calculate_angle(hip_down, knee_mid, hip_mid)
        is_standing = leg_angle < 50

		    # Calculate key angles
        # Torso lean (hip+neck vs vertical)
        torso_lean = calculate_angle(hip_up, neck_mid, hip_mid)

        # Forward head / bending (neck+nose vs vertical)
        head_forward = calculate_angle(neck_up, nose, neck_mid)

        # Nect Angle (neck+nose vs neck+hip)
        neck_angle = calculate_angle(nose, hip_mid, neck_mid)   # angle at neck_mid
        neck_flex = max(0.0, 180.0 - neck_angle)

        # draw on BGR for correct OpenCV colors
        img_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)

        # colors (BGR)
        BODY_COLOR = (0, 255, 255)
        SHOULDER_COLOR = (0, 255, 0)
        NECK_COLOR = (255, 0, 0)
        LEG_COLOR = (255, 0, 255)
        REF_COLOR = (128, 128, 128)

        # lines
        cv2.line(img_bgr, left_shoulder, right_shoulder, SHOULDER_COLOR, 2)
        cv2.line(img_bgr, left_hip, right_hip, BODY_COLOR, 2)
        cv2.line(img_bgr, hip_mid, neck_mid, BODY_COLOR, 2)
        cv2.line(img_bgr, hip_mid, knee_mid, LEG_COLOR, 2)
        cv2.line(img_bgr, hip_mid, hip_up, REF_COLOR, 1, cv2.LINE_AA)
        cv2.line(img_bgr, neck_mid, neck_up, REF_COLOR, 1, cv2.LINE_AA)
        cv2.line(img_bgr, neck_mid, nose, NECK_COLOR, 2)

        # points
        cv2.circle(img_bgr, hip_mid, 4, (0, 0, 255), -1)
        cv2.circle(img_bgr, neck_mid, 4, (0, 255, 0), -1)

        measurements = [
            (f"Leg Angle: {leg_angle:.0f}째 ({'Standing' if is_standing else 'Sitting'})", LEG_COLOR),
            (f"Torso Lean: {torso_lean:.0f}째 ({'Good' if torso_lean < 20 else 'Poor'})", BODY_COLOR),
            (f"Forward Head: {head_forward:.0f}째 ({'Good' if head_forward < 35 else 'Poor'}) (only for side-view)", NECK_COLOR),
            (f"Neck Angle: {neck_flex:.0f}째 ({'Good' if neck_flex < 20 else 'Poor'})", NECK_COLOR),
        ]
        add_legend(img_bgr, measurements, margin=20, font_scale=1.0, text_thickness=2)

        out_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

        metrics = {
            "leg_angle": leg_angle,
            "is_standing": is_standing,
            "torso_lean": torso_lean,
            "head_forward": head_forward,
            "neck_flex": neck_flex,
        }
        return out_rgb, metrics

    except Exception as e:
        return image_rgb, None

"""# Gradio Live Demo"""

from fastrtc import Stream
import time
import cv2
import gradio
from fastrtc import Stream, VideoStreamHandler, AdditionalOutputs

# Define colors
NECK_COLOR = (255, 0, 0)      # Blue
BODY_COLOR = (0, 255, 255)    # Yellow
SHOULDER_COLOR = (0, 255, 0)  # Green
LEG_COLOR = (255, 0, 255)     # Magenta
REFERENCE_COLOR = (128, 128, 128)  # Gray
BENDING_COLOR = (255, 165, 0)  # Orange

def process_frame(frame):
    global last_timestamp
    frame = np.asarray(frame)
    if frame.dtype != np.uint8:
        frame = frame.astype(np.uint8)
    # convert RGB to SRBG
    mediapipe_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)
    # detect the pose
    frame_timestamp = int(time.time_ns()//1000)
    try:
        result = landmarker.detect_for_video(mediapipe_image, frame_timestamp)
    except Exception as e:
        print(e)
        return frame, f"error: {e}"
    # visualize the pose
    annotated_frame = draw_landmarks_on_image(frame, result, thickness=5, circle_radius=8)
    # visualize posture angles
    first_pose = result.pose_landmarks[0] if result.pose_landmarks else None
    annotated_frame, metrics = overlay_posture_angles(annotated_frame, first_pose)
    return annotated_frame, "<posture estimation results>"


with gradio.Blocks() as demo:
    gradio.Markdown("## Posture Detection with MediaPipe PoseLandmarker")
    cam = gradio.Image(label="Live View (Annotated)", sources="webcam", type="numpy")
    out_md = gradio.Markdown()

    cam.stream(
        fn=process_frame,
        inputs=cam,
        outputs=[cam, out_md],
        time_limit=60,
        stream_every=0.25,
        concurrency_limit=1
    )

demo.launch(share=True, debug=True)